{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 basic approaches in Bag of Words\n",
    "https://towardsdatascience.com/3-basic-approaches-in-bag-of-words-which-are-better-than-word-embeddings-c2cbc7398016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "y_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting word occurrence. The reason behind of using this approach is that keyword or important signal will occur again and again. So if the number of occurrence represent the importance of word. More frequency means more importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"In the-state-of-art of the NLP field, Embedding is the \\\n",
    "success way to resolve text related problem and outperform \\\n",
    "Bag of Words ( BoW ). Indeed, BoW introduced limitations \\\n",
    "large feature dimension, sparse representation etc.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer()\n",
    "count_occurs = count_vec.fit_transform([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.feature_extraction.text.VectorizerMixin.build_tokenizer.<locals>.<lambda>(doc)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec.build_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_occur_df = pd.DataFrame((count, word) for word, count in \\\n",
    "                              zip(count_occurs.toarray().tolist()[0], count_vec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_occur_df.columns = ['Word', 'Count']\n",
    "count_occur_df.sort_values('Count', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>of</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>the</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bow</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>way</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Word  Count\n",
       "16   of      3\n",
       "26  the      3\n",
       "3   bow      2\n",
       "0   and      1\n",
       "28  way      1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_occur_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized Count Occurrence\n",
    "High frequency may dominate the result and causing model bias. Normalization can be apply to pipeline easily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hacking Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wm2df(wm, feat_names):\n",
    "    \n",
    "    # create an index for each row\n",
    "    doc_names = ['Doc{:d}'.format(idx) for idx, _ in enumerate(wm)]\n",
    "    df = pd.DataFrame(data=wm.toarray(), index=doc_names, columns=feat_names)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = ['The quick brown fox.','Jumps over the lazy dog!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm = cvec.fit_transform(corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = cvec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Jumps</th>\n",
       "      <th>The</th>\n",
       "      <th>brown</th>\n",
       "      <th>dog</th>\n",
       "      <th>fox</th>\n",
       "      <th>lazy</th>\n",
       "      <th>over</th>\n",
       "      <th>quick</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Jumps  The  brown  dog  fox  lazy  over  quick  the\n",
       "Doc0      0    1      1    0    1     0     0      1    0\n",
       "Doc1      1    0      0    1    0     1     1      0    1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wm2df(wm, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can create a custom vectorize with our own analyzer, preprocessor and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brown</th>\n",
       "      <th>dog</th>\n",
       "      <th>fox</th>\n",
       "      <th>jumped</th>\n",
       "      <th>lazy</th>\n",
       "      <th>over</th>\n",
       "      <th>quick</th>\n",
       "      <th>the</th>\n",
       "      <th>x00021</th>\n",
       "      <th>x0002e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      brown  dog  fox  jumped  lazy  over  quick  the  x00021  x0002e\n",
       "Doc1      1    0    1       0     0     0      1    1       0       1\n",
       "Doc2      0    1    0       1     1     1      0    1       1       0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpora = [\n",
    "    'The quick brown fox&#x0002E;',\n",
    "    'jumped over the lazy dog&#x00021;'\n",
    "]\n",
    "\n",
    "cvec = CountVectorizer()\n",
    "\n",
    "wm = cvec.fit_transform(corpora)\n",
    "\n",
    "tokens = cvec.get_feature_names()\n",
    "\n",
    "pd.DataFrame(data=wm.toarray(), index=['Doc1', 'Doc2'], columns=tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning\n",
    "\n",
    "removing HTML entities as a preprocessing step and lemmatizes the words as the document is tokenized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>.</th>\n",
       "      <th>brown</th>\n",
       "      <th>dog</th>\n",
       "      <th>fox</th>\n",
       "      <th>jump</th>\n",
       "      <th>lazy</th>\n",
       "      <th>over</th>\n",
       "      <th>quick</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      !  .  brown  dog  fox  jump  lazy  over  quick  the\n",
       "Doc0  0  1      1    0    1     0     0     0      1    1\n",
       "Doc1  1  0      0    1    0     1     1     1      0    1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "from html import unescape\n",
    "\n",
    "# create a spaCy tokenizer\n",
    "spacy.load('en')\n",
    "lemmatizer = spacy.lang.en.English()\n",
    "\n",
    "# remove html entities from docs and\n",
    "# set everything to lowercase\n",
    "def my_preprocessor(doc):\n",
    "    return(unescape(doc).lower())\n",
    "\n",
    "# tokenize the doc and lemmatize its tokens\n",
    "def my_tokenizer(doc):\n",
    "    tokens = lemmatizer(doc)\n",
    "    return([token.lemma_ for token in tokens])\n",
    "\n",
    "custom_vec = CountVectorizer(preprocessor=my_preprocessor, tokenizer=my_tokenizer)\n",
    "\n",
    "cwm = custom_vec.fit_transform(corpora)\n",
    "\n",
    "tokens = custom_vec.get_feature_names()\n",
    "\n",
    "wm2df(cwm, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default analyzers all call the preprocessor and tokenizer, but custom analyzers will skip this. N-gram extraction and stop word filtering take place at the analyzer level, so a custom analyzer may have to reproduce these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yevgeniy/anaconda3/envs/dev/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['3', 'far', 'good', 'little', 'make', '\\ufeff1'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>.</th>\n",
       "      <th>brown</th>\n",
       "      <th>brown fox</th>\n",
       "      <th>dog</th>\n",
       "      <th>dog !</th>\n",
       "      <th>fox</th>\n",
       "      <th>fox .</th>\n",
       "      <th>jump</th>\n",
       "      <th>jump lazy</th>\n",
       "      <th>lazy</th>\n",
       "      <th>lazy dog</th>\n",
       "      <th>quick</th>\n",
       "      <th>quick brown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      !  .  brown  brown fox  dog  dog !  fox  fox .  jump  jump lazy  lazy  \\\n",
       "Doc0  0  1      1          1    0      0    1      1     0          0     0   \n",
       "Doc1  1  0      0          0    1      1    0      0     1          1     1   \n",
       "\n",
       "      lazy dog  quick  quick brown  \n",
       "Doc0         0      1            1  \n",
       "Doc1         1      0            0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate a vectorizer with custom preprocessor and tokenizer,\n",
    "# set to remove stop words and extract bigrams\n",
    "custom_vec = CountVectorizer(preprocessor=my_preprocessor,\n",
    "                             tokenizer=my_tokenizer,\n",
    "                             ngram_range=(1,2),\n",
    "                             stop_words='english')\n",
    "cwm = custom_vec.fit_transform(corpora)\n",
    "tokens = custom_vec.get_feature_names()\n",
    "wm2df(cwm, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when a user defined analyzer is used, the build_analyzer method does not call _word_ngrams, which is responsible for removing stop words and extracting n-grams. One way to circumvent this is by creating custom vectorizer classes. The concept is pretty simple, just create a new class inheriting from the base vectorizer and overwrite the build_preprocessor, build_tokenizer and/or build_analyzer methods as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# defines a custom vectorizer class\n",
    "class CustomVectorizer(CountVectorizer): \n",
    "    \n",
    "    # overwrite the build_analyzer method, allowing one to\n",
    "    # create a custom analyzer for the vectorizer\n",
    "    def build_analyzer(self):\n",
    "        \n",
    "        # load stop words using CountVectorizer's built in method\n",
    "        stop_words = self.get_stop_words()\n",
    "        \n",
    "        # create the analyzer that will be returned by this method\n",
    "        def analyser(doc):\n",
    "            \n",
    "            # load spaCy's model for english language\n",
    "            spacy.load('en')\n",
    "            \n",
    "            # instantiate a spaCy tokenizer\n",
    "            lemmatizer = spacy.lang.en.English()\n",
    "            \n",
    "            # apply the preprocessing and tokenzation steps\n",
    "            doc_clean = unescape(doc).lower()\n",
    "            tokens = lemmatizer(doc_clean)\n",
    "            lemmatized_tokens = [token.lemma_ for token in tokens]\n",
    "            \n",
    "            # use CountVectorizer's _word_ngrams built in method\n",
    "            # to remove stop words and extract n-grams\n",
    "            return(self._word_ngrams(lemmatized_tokens, stop_words))\n",
    "        return(analyser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-38-7168b82ecbe2>\u001b[0m(1)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m----> 1 \u001b[0;31m\u001b[0mcnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dog'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>.</th>\n",
       "      <th>brown</th>\n",
       "      <th>brown fox</th>\n",
       "      <th>dog</th>\n",
       "      <th>dog !</th>\n",
       "      <th>fox</th>\n",
       "      <th>fox .</th>\n",
       "      <th>jump</th>\n",
       "      <th>jump lazy</th>\n",
       "      <th>lazy</th>\n",
       "      <th>lazy dog</th>\n",
       "      <th>quick</th>\n",
       "      <th>quick brown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      !  .  brown  brown fox  dog  dog !  fox  fox .  jump  jump lazy  lazy  \\\n",
       "Doc0  0  1      1          1    0      0    1      1     0          0     0   \n",
       "Doc1  1  0      0          0    1      1    0      0     1          1     1   \n",
       "\n",
       "      lazy dog  quick  quick brown  \n",
       "Doc0         0      1            1  \n",
       "Doc1         1      0            0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%debug\n",
    "\n",
    "custom_vec = CustomVectorizer(ngram_range=(1,2),\n",
    "                              stop_words='english')\n",
    "cv_fit = custom_vec.fit_transform(corpora)\n",
    "\n",
    "wm2df(cv_fit, custom_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox&#x0002E;\n",
      "jumped over the lazy dog&#x00021;\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ print(s) for s in corpora]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 1 0 0 0 0 1 1]\n",
      " [1 0 0 0 1 1 0 0 1 1 1 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(cwm.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# faster to perform computation on the sparse array\n",
    "np.asarray(cv_fit.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow(cv_fit, cv):\n",
    "    \n",
    "    word_list = cv.get_feature_names(); \n",
    "    count_list = cv_fit.toarray().sum(axis=0) \n",
    "    \n",
    "    return dict(zip(word_list, count_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = bow(cv_fit, custom_vec)['dog']\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
